{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is on the development of a block-based method for computing distance matrices. The end goal is to produce a method compatible with Fast UniFrac that has reasonable runtime, and dramatically reduced space requirements relative to just computing beta diversity in a single shot. The intuition is that \"blocks\" of the resulting distance matrix can be computed independently, and that computing a \"block\" of samples (e.g., 32 at a time) has dramatically lower memory requirements than computing all of the samples at once (e.g., thousands). The reason for this is that Fast UniFrac produces an internal data structure called the `counts_array` which requires on the order of `O(n * m * log(m))` space, where `n` is the number of samples and `m` is the number of OTUs. By only computing a subset of samples at a time, both `n` and `m` are reduced with the former being a reducing simply by the number of samples being evaluated and the latter on the general case where the examination of fewer samples necessitates representing fewer OTUs in the tree.\n",
    "\n",
    "There still exist optimizations to be performed. The block method is currently setup so that it can be parallelized (as each block is independent), but at the moment, the computation is performed serially. In addition, the block method results in duplicated compute with some positions in the distance matrix being computed `ceil(n / k)` times where `n` is the number of samples and `k` is the block size. However, the compute of an individual pairwise calculation is small relative to the expense of creating a `counts_array`. Note, this suggests using a large `k` but it has been observed that a large `k` can drive runtime up possibly due to needing a larger number of OTUs represented within a block, and thus increasing the expense in space and time of the construction of the `counts_array`. \n",
    "\n",
    "During development, it became apparent that the `TreeNode` is very large to represent in memory, and Fast UniFrac doesn't actually need to operate on it directly. One of the bottlenecks identified was the `TreeNode.shear` method which is used to take a subset of the tree based on the OTUs represented by the samples within a block. More discussion on this new `shear` method can be found [here](https://github.com/wasade/yummy-octo-duck/blob/master/ipynb/Tree%20array%20shearing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import psutil  # pip install psutil\n",
    "\n",
    "from skbio import read, TreeNode, DistanceMatrix\n",
    "from skbio.diversity import beta_diversity\n",
    "from memory_profiler import memory_usage  # pip install memory_profiler\n",
    "from biom import load_table\n",
    "\n",
    "process = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_dist(tree, table, metric, block_size=64):\n",
    "    \"\"\"Perform a block-based computation of a distance matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : TreeNode-like object\n",
    "        A Tree\n",
    "    table : biom\n",
    "        A biom table of the samples and observations\n",
    "    metric : str, one of {unweighted_unifrac, weighted_unifrac}\n",
    "        The method to use\n",
    "    block_size : int\n",
    "        The size of the block in the resulting distance matrix to \n",
    "        compute at a time.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DistanceMatrix\n",
    "        The computed distance matrix\n",
    "    \"\"\"\n",
    "    ids = table.ids()\n",
    "    dmat = np.zeros((len(ids), len(ids)), dtype=float)\n",
    "    dmat_index = {i: idx for i, idx in zip(ids, range(len(ids)))}\n",
    "\n",
    "    # row_start and col_start are relative to the resulting distance matrix\n",
    "    for row_start in range(0, len(ids), block_size):\n",
    "        for col_start in range(row_start, len(ids), block_size):\n",
    "            ### MAP\n",
    "            row_ids = set(ids[row_start:row_start + block_size])\n",
    "            col_ids = set(ids[col_start:col_start + block_size])\n",
    "            ids_to_keep = row_ids.union(col_ids)\n",
    "\n",
    "            block = table.filter(ids_to_keep, inplace=False)\n",
    "            block.filter(lambda v, i, md: v.sum() > 0, axis='observation')\n",
    "            block_ids = block.ids()\n",
    "            block_otu_ids = block.ids(axis='observation')\n",
    "            block_tree = tree.shear(block_otu_ids)\n",
    "            block_matrix = block.matrix_data.astype(int).T.toarray()\n",
    "\n",
    "            block_dmat = beta_diversity(metric, block_matrix, block_ids,\n",
    "                                        tree=block_tree, otu_ids=block_otu_ids,\n",
    "                                        validate=False)\n",
    "            ### END MAP\n",
    "\n",
    "            ### REDUCE\n",
    "            for i in block_dmat.ids:\n",
    "                i_idx = block_dmat.index(i)\n",
    "                for j in block_dmat.ids[i_idx:]:\n",
    "                    j_idx = block_dmat.index(j)\n",
    "                    dmat[dmat_index[i], dmat_index[j]] = block_dmat.data[i_idx, j_idx]\n",
    "            ### END REDUCE\n",
    "            \n",
    "    return DistanceMatrix(dmat + dmat.T, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shear(indexed, to_keep):\n",
    "    \"\"\"Shear off nodes from a tree array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    indexed : dict\n",
    "        The result of TreeNode.to_array\n",
    "    to_keep : set\n",
    "        The tip IDs of the tree to keep\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A TreeNode.to_array like dict with the exception that \"id_index\" is not\n",
    "        provided, and any extraneous attributes formerly included are not \n",
    "        passed on.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Unlike TreeNode.shear, this method does not prune (i.e., collapse single\n",
    "    descendent nodes). This is an open development target.\n",
    "    \n",
    "    This method assumes that to_keep is a subset of names in the tree.\n",
    "    \n",
    "    The order of the nodes remains unchanged.\n",
    "    \"\"\"\n",
    "    # nodes to keep mask\n",
    "    mask = np.zeros(len(indexed['id']), dtype=np.bool)\n",
    "\n",
    "    # set any tips marked \"to_keep\"\n",
    "    tips_to_keep = [i for i, n in enumerate(indexed['name']) if n in to_keep]\n",
    "    mask[np.asarray(tips_to_keep)] = True\n",
    "\n",
    "    # perform a post-order traversal and identify any nodes that should be \n",
    "    # retained\n",
    "    new_child_index = []\n",
    "    for node_idx, child_left, child_right in indexed['child_index']:\n",
    "        being_kept = mask[child_left:child_right + 1]\n",
    "\n",
    "        # NOTE: the second clause is an explicit test to keep the root node. This \n",
    "        # may not be necessary and may be a remenant of mucking around.\n",
    "        if being_kept.sum() >= 1 or node_idx == indexed['id'][-1]:\n",
    "            mask[node_idx] = True\n",
    "\n",
    "    # we now know what nodes to keep, so we can create new IDs for assignment\n",
    "    new_ids = np.arange(mask.sum(), dtype=int)\n",
    "    \n",
    "    # construct a map that associates old node IDs to the new IDs\n",
    "    id_map = {i_old: i_new for i_old, i_new in zip(indexed['id'][mask], new_ids)}\n",
    "\n",
    "    # perform another post-order traversal to construct the new child index arrays\n",
    "    # which provide index positions of the desecendents of a given internal node.\n",
    "    for node_idx, child_left, child_right in indexed['child_index']:\n",
    "        being_kept = mask[child_left:child_right + 1]\n",
    "\n",
    "        # NOTE: the second clause is an explicit test to keep the root node. This \n",
    "        # may not be necessary and may be a remenant of mucking around.\n",
    "        if being_kept.sum() >= 1 or node_idx == indexed['id'][-1]:\n",
    "            new_id = id_map[node_idx]\n",
    "            child_indices = indexed['id'][child_left:child_right + 1][being_kept]\n",
    "            left_child = id_map[child_indices[0]]\n",
    "            right_child = id_map[child_indices[-1]]\n",
    "            new_child_index.append([new_id, left_child, right_child])\n",
    "\n",
    "    new_child_index = np.asarray(new_child_index)\n",
    "\n",
    "    return {'child_index': new_child_index,\n",
    "            'length': indexed['length'][mask],\n",
    "            'name': indexed['name'][mask],\n",
    "            'id': new_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MockTreeNode(object):\n",
    "    def __init__(self, original_tree_array):\n",
    "        self.original_tree_array = original_tree_array\n",
    "\n",
    "    def to_array(self, nan_length_value=0.0):\n",
    "        self.original_tree_array['length'][np.isnan(self.original_tree_array['length'])] = nan_length_value\n",
    "        return self.original_tree_array\n",
    "\n",
    "    def shear(self, to_keep):\n",
    "        return MockTreeNode(shear(self.original_tree_array, to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench(tree, table, number_otus, number_samples, block_size, metric):\n",
    "    \"\"\"Benchmark the block and regular beta diversity methods\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : path\n",
    "        File path to the tree to load\n",
    "    table : path\n",
    "        File path to the table to load\n",
    "    number_otus : int\n",
    "        Number of OTUs to use in the test\n",
    "    number_samples : int\n",
    "        Number of samples to use in the test\n",
    "    block_size : int\n",
    "        The blocksize to use for the test\n",
    "    metric : str, {unweighted_unifrac, weighted_unifrac}\n",
    "    \"\"\"\n",
    "    # the time to read the tree and BIOM table\n",
    "    start = time()\n",
    "    tree = read(tree, into=TreeNode)\n",
    "    table = load_table(table)\n",
    "\n",
    "    for node in tree.traverse(include_self=False):\n",
    "        if node.length is None:\n",
    "            node.length = 0.0\n",
    "    \n",
    "    # aggressively clean up leaky variables so the original tree can be freed\n",
    "    del node\n",
    "    \n",
    "    # get sample subset\n",
    "    sample_ids = table.ids()\n",
    "\n",
    "    # samples must have at least 1000 sequences\n",
    "    sample_ids = [i for i, v in zip(sample_ids, table.sum(axis='sample')) if v >= 1000]\n",
    "    shuffle(sample_ids)\n",
    "    sample_ids = sample_ids[:number_samples]\n",
    "    table.filter(sample_ids)\n",
    "    \n",
    "    # observations must exist in at least .1% of samples\n",
    "    table.filter(lambda v, i, md: ((v != 0).sum() / len(sample_ids)) >= 0.001, axis='observation')\n",
    "\n",
    "    # get otu subset of the tree\n",
    "    table_obs_ids = table.ids(axis='observation')\n",
    "    table_obs_idx_lookup = {i: idx for idx, i in enumerate(table_obs_ids)}\n",
    "    otu_ids = [n.name for n in tree.tips()]\n",
    "    otu_ids = list(set(otu_ids).intersection(set(table_obs_ids)))  # make sure OTUs overlap with table\n",
    "    shuffle(otu_ids)\n",
    "    otu_ids = otu_ids[:number_otus]\n",
    "    \n",
    "    # construct a MockTreeNode using skbio's TreeNode.shear method\n",
    "    tree_array = MockTreeNode(tree.shear(otu_ids).to_array())\n",
    "    \n",
    "    # delete unnecessary references to the tree, drop the tree and request a cleanup\n",
    "    del tree_array.original_tree_array['id_index']\n",
    "    del tree\n",
    "    import gc; gc.collect()\n",
    "    \n",
    "    # remove excess OTUs from the table\n",
    "    table.filter(otu_ids, axis='observation')\n",
    "\n",
    "    # remove samples without any OTUs (hopefully a small number...)\n",
    "    table.filter(lambda v, i, md: v.sum() > 0)\n",
    "\n",
    "    print(\"# spinuptime: %f\" % (time() - start))\n",
    "    print(\"# spinupmem: %f\" % (process.memory_info().rss / 2**20))\n",
    "\n",
    "    # if we dropped more than 10% of desired samples do to filtering about, let's bail\n",
    "    if ((number_samples - len(table.ids())) / float(number_samples)) > 0.1:\n",
    "        print(number_samples)\n",
    "        print(len(table.ids()))\n",
    "        raise ValueError(\"Dropped too many samples!\")\n",
    "\n",
    "    # run the block method\n",
    "    args = (tree_array, table, metric, block_size)\n",
    "    block_start = time()\n",
    "    (block_usage, block_result) = memory_usage((block_dist, args),\n",
    "                                               interval=2, max_usage=True,\n",
    "                                               retval=True)\n",
    "    block_time = time() - block_start\n",
    "\n",
    "    print(\"#number_otus\\tnumber_samples\\tblocksize\\truntime\\tpeakmem\\tmethod\\n\")\n",
    "    print('\\t'.join([str(i) for i in [number_otus, number_samples,\n",
    "                                      block_size, block_time,\n",
    "                                      block_usage[-1], 'block']]))\n",
    "    \n",
    "    # run the normal method\n",
    "    test_matrix = table.matrix_data.astype(int).T.toarray()\n",
    "    args = (metric, test_matrix, table.ids())\n",
    "    kwargs = {'tree': tree_array, 'otu_ids':table.ids(axis='observation'),\n",
    "                  'validate': False}\n",
    "    normal_start = time()\n",
    "    (usage, result) = memory_usage((beta_diversity, args, kwargs),\n",
    "                                       interval=2, max_usage=True, retval=True)\n",
    "    normal_time = time() - normal_start\n",
    "\n",
    "    print('\\t'.join([str(i) for i in [number_otus, number_samples,\n",
    "                                      'NA', normal_time, usage[-1],\n",
    "                                      'regular']]))\n",
    "\n",
    "    if not np.allclose(block_result.data, result.data):\n",
    "        print(block_result.ids == result.ids)\n",
    "        print(block_result.data[:5, :5])\n",
    "        print(result.data[:5, :5])\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# spinuptime: 20.920741\n",
      "# spinupmem: 270.000000\n",
      "#number_otus\tnumber_samples\tblocksize\truntime\tpeakmem\tmethod\n",
      "\n",
      "1000\t500\t32\t11.583980083465576\t275.95703125\tblock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/blockdist/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t500\tNA\t6.350912094116211\t305.48828125\tregular\n"
     ]
    }
   ],
   "source": [
    "gg_tree = '/Users/daniel/miniconda3/envs/qiime191/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/trees/97_otus.tree'\n",
    "ag_table = '/Users/daniel/rs/American-Gut/data/AG/AG_even1k.biom'\n",
    "number_otus = 1000\n",
    "number_samples = 500\n",
    "block_size = 32\n",
    "metric = 'unweighted_unifrac'\n",
    "\n",
    "bench(gg_tree, ag_table, number_otus, number_samples, block_size, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
